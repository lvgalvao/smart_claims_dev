# Smart Claims - Projeto Databricks

## üìã Sobre o Projeto

**Smart Claims** √© um projeto de an√°lise e processamento de sinistros (claims) utilizando a plataforma **Databricks** e a arquitetura moderna de **Lakehouse** com **Unity Catalog** (Lakeflow). O projeto implementa o padr√£o **Medallion Architecture** para garantir qualidade, rastreabilidade e governan√ßa de dados.

### Objetivos do Projeto

- Processar e analisar dados de sinistros de forma eficiente e escal√°vel
- Implementar governan√ßa de dados com Unity Catalog
- Criar uma arquitetura de dados em camadas (Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold)
- Facilitar an√°lises, relat√≥rios e modelos de Machine Learning sobre sinistros

---

## üèóÔ∏è Arquitetura de Dados

O projeto segue o padr√£o **Medallion Architecture**, organizando dados em camadas progressivas:

```mermaid
graph TB
    subgraph "smart_claims_dev (CATALOG)"
        L[00_landing<br/>Zona de Recep√ß√£o<br/>RAW Files]
        B[01_bronze<br/>Dados Brutos<br/>Imut√°veis]
        S[02_silver<br/>Dados Curados<br/>Validados]
        G[03_gold<br/>Dados Agregados<br/>Otimizados]
        D[default<br/>Schema Padr√£o]
        I[information_schema<br/>Metadados Sistema]
    end
    
    L -->|Ingest√£o| B
    B -->|Limpeza & Valida√ß√£o| S
    S -->|Agrega√ß√£o & Modelagem| G
    
    style L fill:#ff9999
    style B fill:#ffcc99
    style S fill:#99ccff
    style G fill:#99ff99
```

### Estrutura Hier√°rquica do Cat√°logo

```mermaid
graph TD
    CAT[smart_claims_dev<br/>CATALOG]
    
    CAT --> SCH1[00_landing<br/>SCHEMA]
    CAT --> SCH2[01_bronze<br/>SCHEMA]
    CAT --> SCH3[02_silver<br/>SCHEMA]
    CAT --> SCH4[03_gold<br/>SCHEMA]
    CAT --> SCH5[default<br/>SCHEMA]
    
    SCH1 --> V1[claims<br/>VOLUME]
    SCH1 --> V2[sql_server<br/>VOLUME]
    SCH1 --> V3[telematics<br/>VOLUME]
    SCH1 --> V4[training_imgs<br/>VOLUME]
    
    SCH2 --> T1[claim<br/>TABLE]
    SCH2 --> T2[customer<br/>TABLE]
    SCH2 --> T3[policy<br/>TABLE]
    SCH2 --> T4[telematics<br/>TABLE]
    
    SCH3 --> T5[claim<br/>TABLE]
    SCH3 --> T6[customer<br/>TABLE]
    SCH3 --> T7[policy<br/>TABLE]
    SCH3 --> T8[telematics<br/>TABLE]
    
    SCH4 --> T9[aggregated_telematics<br/>TABLE]
    SCH4 --> T10[customer_claim_policy<br/>TABLE]
    SCH4 --> T11[customer_claim_policy_telematics<br/>TABLE]
    
    style CAT fill:#4a90e2,color:#fff
    style SCH1 fill:#ff9999
    style SCH2 fill:#ffcc99
    style SCH3 fill:#99ccff
    style SCH4 fill:#99ff99
```

### Descri√ß√£o das Camadas

| Camada | Prop√≥sito | Reten√ß√£o | Formato |
|--------|-----------|----------|---------|
| **00_landing** | Recep√ß√£o inicial de dados de sistemas externos | 7 dias | RAW (CSV, Parquet, Imagens) |
| **01_bronze** | Preserva√ß√£o imut√°vel dos dados originais | 365 dias | Delta Lake (append-only) |
| **02_silver** | Dados limpos, validados e enriquecidos | 730 dias | Delta Lake (schema definido) |
| **03_gold** | Dados agregados e otimizados para consumo final | 2555 dias | Delta Lake (otimizado, particionado) |

### Fluxo Completo de Dados

```mermaid
flowchart LR
    subgraph "Fontes de Dados"
        SQL[SQL Server<br/>CSV Files]
        TEL[Telemetria<br/>Parquet Files]
        IMG[Imagens<br/>PNG/JPG]
    end
    
    subgraph "00_landing - Volumes"
        V1[claims<br/>Volume]
        V2[sql_server<br/>Volume]
        V3[telematics<br/>Volume]
        V4[training_imgs<br/>Volume]
    end
    
    subgraph "01_bronze - Tabelas Raw"
        B1[claim<br/>TABLE]
        B2[customer<br/>TABLE]
        B3[policy<br/>TABLE]
        B4[telematics<br/>TABLE]
    end
    
    subgraph "02_silver - Tabelas Curadas"
        S1[claim<br/>TABLE]
        S2[customer<br/>TABLE]
        S3[policy<br/>TABLE]
        S4[telematics<br/>TABLE]
    end
    
    subgraph "03_gold - Tabelas Agregadas"
        G1[aggregated_telematics<br/>TABLE]
        G2[customer_claim_policy<br/>TABLE]
        G3[customer_claim_policy_telematics<br/>TABLE]
    end
    
    SQL --> V1
    SQL --> V2
    TEL --> V3
    IMG --> V4
    
    V1 --> B1
    V2 --> B2
    V2 --> B3
    V3 --> B4
    
    B1 --> S1
    B2 --> S2
    B3 --> S3
    B4 --> S4
    
    S4 --> G1
    S1 --> G2
    S2 --> G2
    S3 --> G2
    G1 --> G3
    G2 --> G3
    
    style V1 fill:#ffcccc
    style V2 fill:#ffcccc
    style V3 fill:#ffcccc
    style V4 fill:#ffcccc
    style B1 fill:#ffe6cc
    style B2 fill:#ffe6cc
    style B3 fill:#ffe6cc
    style B4 fill:#ffe6cc
    style S1 fill:#ccf2ff
    style S2 fill:#ccf2ff
    style S3 fill:#ccf2ff
    style S4 fill:#ccf2ff
    style G1 fill:#ccffcc
    style G2 fill:#ccffcc
    style G3 fill:#ccffcc
```

---

## üìÅ Estrutura do Reposit√≥rio

```text
smart_claims_dev/
‚îú‚îÄ‚îÄ README.md                              # Este arquivo - documenta√ß√£o principal
‚îú‚îÄ‚îÄ EXPLICACAO_CATALOG.md                  # Documenta√ß√£o detalhada sobre Unity Catalog
‚îú‚îÄ‚îÄ 01_create_catalog_and_schemas.ipynb   # Notebook: Task_001 - Criar cat√°logo e schemas
‚îú‚îÄ‚îÄ 02_create_volumes_and_load_data.ipynb  # Notebook: Task_002 - Criar volumes
‚îú‚îÄ‚îÄ ingest_sql/                            # Scripts SQL para pipelines DLT
‚îÇ   ‚îú‚îÄ‚îÄ README.md                          # Documenta√ß√£o dos pipelines
‚îÇ   ‚îú‚îÄ‚îÄ source_to_bronze/                  # Ingest√£o: Volumes ‚Üí Bronze
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_claim.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_customers.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_policies.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ get_telematics.sql
‚îÇ   ‚îú‚îÄ‚îÄ bronze_to_silver/                  # Transforma√ß√£o: Bronze ‚Üí Silver
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_claim.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_customer.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_policy.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clean_telematics.sql
‚îÇ   ‚îî‚îÄ‚îÄ silver_to_gold/                    # Agrega√ß√£o: Silver ‚Üí Gold
‚îÇ       ‚îú‚îÄ‚îÄ aggregated_telematics.sql
‚îÇ       ‚îú‚îÄ‚îÄ customer_claim_policy.sql
‚îÇ       ‚îî‚îÄ‚îÄ customer_claim_policy_telematics.sql
‚îî‚îÄ‚îÄ data/                                  # Dados de exemplo
    ‚îú‚îÄ‚îÄ claims/                            # Imagens e metadata de sinistros
    ‚îú‚îÄ‚îÄ sql_server/                        # CSV do SQL Server
    ‚îÇ   ‚îú‚îÄ‚îÄ claims.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ customers.csv
    ‚îÇ   ‚îî‚îÄ‚îÄ policies.csv
    ‚îú‚îÄ‚îÄ telematics/                        # Arquivos Parquet de telemetria
    ‚îî‚îÄ‚îÄ training_imgs/                     # Imagens PNG para treinamento ML
```

---

## ‚úÖ Task_001 - Cria√ß√£o do Cat√°logo e Schemas

### Objetivo

Criar a estrutura base do projeto no Databricks utilizando **Unity Catalog**, incluindo o cat√°logo principal e todos os schemas necess√°rios para implementar a arquitetura Medallion.

### O que foi Implementado

#### 1. **Cria√ß√£o do Cat√°logo `smart_claims_dev`**

Foi criado um cat√°logo completo no Unity Catalog conforme implementado no notebook:

```sql
CREATE CATALOG IF NOT EXISTS smart_claims_dev
COMMENT 'Cat√°logo principal para o projeto Smart Claims - Ambiente de Desenvolvimento'
```

Seguido pelo comando para usar o cat√°logo:

```sql
USE CATALOG smart_claims_dev
```

**Exemplos concretos do que isso proporciona:**

- ‚úÖ Isolamento l√≥gico de todos os dados do projeto Smart Claims
- ‚úÖ Coment√°rio descritivo para documenta√ß√£o e rastreabilidade
- ‚úÖ Base para compartilhamento entre workspaces/organiza√ß√µes
- ‚úÖ Governan√ßa centralizada de permiss√µes e pol√≠ticas

#### 2. **Cria√ß√£o dos 6 Schemas**

Cada schema foi criado com propriedades espec√≠ficas e coment√°rios descritivos:

##### **00_landing** - Zona de Recep√ß√£o

```sql
CREATE SCHEMA IF NOT EXISTS smart_claims_dev.00_landing
COMMENT 'Zona de landing - recep√ß√£o de dados brutos de sistemas externos'
```

**Exemplo de uso:** Tabelas como `raw_claims_api`, `raw_policies_export`, `raw_customer_data` receberiam dados diretamente de APIs ou sistemas externos.

##### **01_bronze** - Preserva√ß√£o de Dados Brutos

```sql
CREATE SCHEMA IF NOT EXISTS smart_claims_dev.01_bronze
COMMENT 'Camada Bronze - dados brutos preservados de forma imut√°vel'
```

**Exemplo de uso:** Tabelas como `bronze.claims_raw`, `bronze.policies_raw`, `bronze.customers_raw` manteriam uma c√≥pia imut√°vel de todos os dados originais, permitindo auditoria e reprocessamento hist√≥rico.

##### **02_silver** - Dados Curados

```sql
CREATE SCHEMA IF NOT EXISTS smart_claims_dev.02_silver
COMMENT 'Camada Silver - dados limpos, validados e enriquecidos'
```

**Exemplo de uso:** Tabelas como `silver.claims_clean`, `silver.claims_enriched`, `silver.customers_master` conteriam dados ap√≥s:

- Valida√ß√£o de tipos e formatos
- Remo√ß√£o de duplicatas
- Enriquecimento com dados de refer√™ncia
- Normaliza√ß√£o de estruturas

##### **03_gold** - Dados para Consumo Final

```sql
CREATE SCHEMA IF NOT EXISTS smart_claims_dev.03_gold
COMMENT 'Camada Gold - dados agregados e modelados para consumo final'
```

**Exemplo de uso:** Tabelas como `gold.claims_by_month`, `gold.claims_summary`, `gold.customer_claims_facts` seriam otimizadas para:

- Dashboards executivos
- Modelos de Machine Learning
- Relat√≥rios anal√≠ticos
- Star schemas para BI tools

##### **default** - Schema Padr√£o

Schema padr√£o do cat√°logo para objetos que n√£o requerem organiza√ß√£o espec√≠fica por camada.

##### **information_schema** - Metadados do Sistema

Schema autom√°tico do Unity Catalog que cont√©m metadados sobre todos os objetos do cat√°logo (tabelas, views, fun√ß√µes, etc.).

#### 3. **Comandos de Verifica√ß√£o Inclu√≠dos**

O notebook inclui comandos para valida√ß√£o da estrutura criada, cada um em uma c√©lula separada:

```sql
-- Listar cat√°logos
SHOW CATALOGS LIKE 'smart_claims*'

-- Listar schemas no cat√°logo
SHOW SCHEMAS IN smart_claims_dev

-- Descrever cat√°logo
DESCRIBE CATALOG smart_claims_dev

-- Descrever schema espec√≠fico
DESCRIBE SCHEMA smart_claims_dev.00_landing
```

### Arquivos Gerados

1. **`01_create_catalog_and_schemas.ipynb`**
   - Notebook Databricks completo com c√©lulas SQL
   - Cada comando SQL em uma c√©lula separada para execu√ß√£o individual
   - Coment√°rios markdown explicando cada comando e sua fun√ß√£o
   - Comandos de verifica√ß√£o inclu√≠dos
   - Idempotente (pode ser executado m√∫ltiplas vezes sem erro)
   - Pronto para execu√ß√£o no Databricks Workspace

2. **`EXPLICACAO_CATALOG.md`**
   - Documenta√ß√£o completa sobre Unity Catalog
   - Explica√ß√£o detalhada de cada comando SQL
   - Descri√ß√£o da Medallion Architecture
   - Guia de melhores pr√°ticas

### Como Executar

1. **Importe o notebook no Databricks:**
   - No workspace, v√° em **Workspace** ‚Üí **Import**
   - Selecione o arquivo `01_create_catalog_and_schemas.ipynb`
   - Ou arraste e solte o arquivo na interface

2. **Execute as c√©lulas sequencialmente:**
   - Execute as c√©lulas markdown (apenas para leitura)
   - Execute as c√©lulas SQL uma por uma ou use "Run All"
   - Cada comando SQL est√° em uma c√©lula separada

3. **Verifique os resultados:**
   - Execute as c√©lulas de verifica√ß√£o (SHOW CATALOGS, SHOW SCHEMAS, DESCRIBE)
   - Confirme que o cat√°logo e todos os schemas foram criados com sucesso

### Resultado Esperado

Ap√≥s a execu√ß√£o bem-sucedida do notebook, voc√™ ter√°:

- ‚úÖ 1 cat√°logo criado: `smart_claims_dev` com coment√°rio descritivo
- ‚úÖ 4 schemas principais criados (00_landing, 01_bronze, 02_silver, 03_gold)
- ‚úÖ Schema `default` dispon√≠vel para uso geral
- ‚úÖ Schema `information_schema` criado automaticamente pelo Unity Catalog
- ‚úÖ Estrutura completa para iniciar ingest√£o de dados
- ‚úÖ Base s√≥lida para implementar pipelines de dados seguindo Medallion Architecture
- ‚úÖ Governan√ßa de dados configurada com Unity Catalog

### Estrutura do Notebook

O notebook est√° organizado em 3 partes principais:

1. **Parte 1: Criar o Cat√°logo** (2 c√©lulas SQL)
   - CREATE CATALOG
   - USE CATALOG

2. **Parte 2: Criar os Schemas** (4 c√©lulas SQL)
   - CREATE SCHEMA para cada camada (00_landing, 01_bronze, 02_silver, 03_gold)

3. **Parte 3: Verifica√ß√£o** (4 c√©lulas SQL)
   - SHOW CATALOGS
   - SHOW SCHEMAS
   - DESCRIBE CATALOG
   - DESCRIBE SCHEMA

### Pipeline Completo de Dados

```mermaid
sequenceDiagram
    participant Ext as Fontes Externas
    participant Land as 00_landing<br/>(Volumes)
    participant Brz as 01_bronze<br/>(Raw Tables)
    participant Sil as 02_silver<br/>(Clean Tables)
    participant Gld as 03_gold<br/>(Aggregated)
    
    Ext->>Land: Upload CSV/Parquet/Imagens
    Land->>Brz: Pipeline: source_to_bronze<br/>(cloud_files)
    Brz->>Sil: Pipeline: bronze_to_silver<br/>(Transforma√ß√µes)
    Sil->>Gld: Pipeline: silver_to_gold<br/>(Joins & Agrega√ß√µes)
    Gld->>Gld: Tabelas prontas para<br/>Dashboards, ML, BI
```

### Pr√≥ximos Passos

Ap√≥s concluir a Task_001, as pr√≥ximas etapas incluem:

- **Task_002**: Criar volumes no schema 00_landing e carregar arquivos (‚úÖ Conclu√≠da)
- **Task_003**: Executar pipelines de ingest√£o (source_to_bronze) ‚úÖ Conclu√≠da
- **Task_004**: Executar transforma√ß√µes (bronze_to_silver) ‚úÖ Conclu√≠da
- **Task_005**: Executar agrega√ß√µes (silver_to_gold) ‚úÖ Conclu√≠da
- **Task_006**: Configurar permiss√µes e roles (data engineers, analysts, etc.)
- **Task_007**: Configurar monitoramento, alertas e qualidade de dados

---

## ‚úÖ Task_002 - Criar Volumes e Carregar Dados no Schema 00_landing

### Objetivo

Criar volumes no schema `00_landing` para armazenar os arquivos CSV brutos da pasta `data/` e implementar m√©todos para fazer upload desses arquivos para os volumes do Unity Catalog.

### Arquivos Envolvidos

A Task_002 trabalha com 3 arquivos CSV brutos que cont√™m os dados do projeto Smart Claims:

- **`claims.csv`** - Dados de sinistros (claims) com aproximadamente 12.990 registros
  - Cont√©m informa√ß√µes sobre sinistros: data, valor, tipo de colis√£o, severidade, etc.
  - Colunas: `claim_no`, `policy_no`, `claim_date`, `total`, `collision_type`, `severity`, etc.
  
- **`customers.csv`** - Dados de clientes com aproximadamente 7.060 registros
  - Cont√©m informa√ß√µes demogr√°ficas dos clientes
  - Colunas: `customer_id`, `date_of_birth`, `borough`, `neighborhood`, `zip_code`, `name`
  
- **`policies.csv`** - Dados de ap√≥lices com aproximadamente 12.230 registros
  - Cont√©m informa√ß√µes sobre ap√≥lices de seguro
  - Colunas: `POLICY_NO`, `CUST_ID`, `POLICYTYPE`, `MAKE`, `MODEL`, `SUM_INSURED`, `PREMIUM`, etc.

### O que foi Implementado

#### 1. **Cria√ß√£o de 3 Volumes no Schema 00_landing**

Foram criados volumes para organizar cada tipo de arquivo CSV de forma isolada:

```sql
-- Volume para dados de sinistros (claims)
CREATE VOLUME IF NOT EXISTS smart_claims_dev.`00_landing`.claims_volume
COMMENT 'Volume para armazenar arquivos CSV de sinistros';
```

**Exemplo de uso:** O volume `claims_volume` armazena o arquivo `claims.csv` que cont√©m todos os registros de sinistros recebidos. Este volume serve como ponto de entrada para dados brutos que ser√£o posteriormente processados e movidos para a camada Bronze.

```sql
-- Volume para dados de clientes (customers)
CREATE VOLUME IF NOT EXISTS smart_claims_dev.`00_landing`.customers_volume
COMMENT 'Volume para armazenar arquivos CSV de clientes';
```

**Exemplo de uso:** O volume `customers_volume` armazena o arquivo `customers.csv` com informa√ß√µes dos clientes. Este volume permite armazenar dados de refer√™ncia que ser√£o usados para enriquecer outras tabelas nas camadas seguintes.

```sql
-- Volume para dados de ap√≥lices (policies)
CREATE VOLUME IF NOT EXISTS smart_claims_dev.`00_landing`.policies_volume
COMMENT 'Volume para armazenar arquivos CSV de ap√≥lices';
```

**Exemplo de uso:** O volume `policies_volume` armazena o arquivo `policies.csv` com informa√ß√µes sobre as ap√≥lices de seguro. Este volume cont√©m dados de contrato que ser√£o vinculados aos sinistros no processamento posterior.

**Benef√≠cios concretos:**

- ‚úÖ **Organiza√ß√£o**: Cada tipo de dado tem seu pr√≥prio container, facilitando gest√£o e localiza√ß√£o
- ‚úÖ **Governan√ßa**: Controle de acesso granular atrav√©s do Unity Catalog (permiss√µes por volume)
- ‚úÖ **Rastreabilidade**: Hist√≥rico completo de arquivos recebidos na zona de landing
- ‚úÖ **Isolamento**: Falhas ou problemas em um volume n√£o afetam os outros
- ‚úÖ **Auditoria**: Logs de acesso e modifica√ß√µes para cada volume separadamente

#### 2. **Comandos de Verifica√ß√£o de Volumes**

O notebook inclui comandos SQL para verificar a cria√ß√£o dos volumes:

```sql
-- Listar todos os volumes no schema 00_landing
SHOW VOLUMES IN smart_claims_dev.`00_landing`;
```

**Exemplo de sa√≠da esperada:**

```text
volume_name      | volume_type | provider | storage_location
-----------------|-------------|----------|------------------
claims_volume    | MANAGED     | ...      | /Volumes/...
customers_volume | MANAGED     | ...      | /Volumes/...
policies_volume   | MANAGED     | ...      | /Volumes/...
```

#### 3. **M√©todos de Upload de Arquivos Implementados**

O notebook fornece **3 m√©todos diferentes** para fazer upload dos arquivos CSV para os volumes, atendendo diferentes cen√°rios de uso:

##### **M√©todo 1: Upload via UI do Databricks (Recomendado)**

O m√©todo mais simples e direto para usu√°rios:

1. No Databricks Workspace, navegue at√©: **Catalog** ‚Üí **smart_claims_dev** ‚Üí **00_landing**
2. Clique no volume desejado (ex: `claims_volume`)
3. Clique em **Upload** ou **Add files**
4. Selecione o arquivo CSV correspondente do seu sistema local
5. Repita para todos os volumes

**Vantagens:**

- Interface gr√°fica intuitiva
- Valida√ß√£o autom√°tica de arquivos
- Progresso visual do upload
- Sem necessidade de c√≥digo

##### **M√©todo 2: C√≥pia de Reposit√≥rio Git para Volumes**

Se o projeto est√° conectado a um reposit√≥rio Git no Databricks:

```python
# Caminho do reposit√≥rio Git no Databricks
repo_path = "/Workspace/Repos/your_username/smart_claims_dev/data"

# Copiar arquivos do reposit√≥rio para os volumes
dbutils.fs.cp(f"{repo_path}/claims.csv", 
              "/Volumes/smart_claims_dev/00_landing/claims_volume/claims.csv")
dbutils.fs.cp(f"{repo_path}/customers.csv", 
              "/Volumes/smart_claims_dev/00_landing/customers_volume/customers.csv")
dbutils.fs.cp(f"{repo_path}/policies.csv", 
              "/Volumes/smart_claims_dev/00_landing/policies_volume/policies.csv")
```

**Vantagens:**

- Automatiza√ß√£o via c√≥digo
- Ideal para integra√ß√£o CI/CD
- Versionamento atrav√©s do Git
- Consist√™ncia entre ambientes

##### **M√©todo 3: Upload via DBFS e C√≥pia para Volumes**

Se voc√™ precisa fazer upload tempor√°rio via DBFS primeiro:

```python
# Passo 1: Fazer upload para DBFS (via UI ou dbutils.fs.put())
# Passo 2: Copiar de DBFS para os volumes
dbutils.fs.cp("dbfs:/FileStore/uploads/claims.csv", 
              "/Volumes/smart_claims_dev/00_landing/claims_volume/claims.csv")
```

**Vantagens:**

- Flexibilidade para diferentes origens de dados
- √ötil para dados grandes que precisam de upload incremental
- Permite transforma√ß√µes intermedi√°rias no DBFS

#### 4. **Comandos de Verifica√ß√£o de Arquivos**

Ap√≥s o upload, o notebook inclui c√≥digo Python para verificar os arquivos:

```python
# Listar arquivos em cada volume
files = dbutils.fs.ls("/Volumes/smart_claims_dev/00_landing/claims_volume/")
for file in files:
    print(f"  - {file.name} ({file.size} bytes)")
```

**Exemplo de sa√≠da esperada:**

```text
=== Arquivos no volume claims_volume ===
  - claims.csv (2847392 bytes)

=== Arquivos no volume customers_volume ===
  - customers.csv (456123 bytes)

=== Arquivos no volume policies_volume ===
  - policies.csv (892456 bytes)
```

#### 5. **Leitura dos CSV como DataFrames Spark (Opcional)**

O notebook inclui exemplos de como ler os arquivos CSV dos volumes diretamente como DataFrames Spark:

```python
# Ler arquivo CSV do volume como DataFrame
df_claims = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("/Volumes/smart_claims_dev/00_landing/claims_volume/claims.csv")

# Visualizar dados
df_claims.show(5, truncate=False)
print(f"Total de registros: {df_claims.count()}")
```

**Exemplo de uso:** Esta funcionalidade permite validar os dados antes de criar tabelas Delta, verificar a qualidade dos dados e fazer an√°lises explorat√≥rias.

### Arquivo Gerado

**`02_create_volumes_and_load_data.ipynb`**

Notebook Databricks completo contendo:

- ‚úÖ **C√©lulas SQL** para criar os 3 volumes
- ‚úÖ **C√©lulas Python** com c√≥digo para upload via `dbutils`
- ‚úÖ **M√∫ltiplos m√©todos** de upload para diferentes cen√°rios
- ‚úÖ **Comandos de verifica√ß√£o** para validar cria√ß√£o e upload
- ‚úÖ **Exemplos de leitura** dos CSV como DataFrames Spark
- ‚úÖ **Documenta√ß√£o inline** explicando cada passo
- ‚úÖ **Tratamento de erros** com mensagens informativas

### Como Executar

#### Passo a Passo Completo

1. **Importe o notebook no Databricks:**
   - No workspace, v√° em **Workspace** ‚Üí **Import**
   - Selecione `02_create_volumes_and_load_data.ipynb`
   - Ou arraste e solte o arquivo

2. **Execute as c√©lulas SQL (Partes 1-3):**
   - Execute as c√©lulas para garantir que o cat√°logo existe
   - Execute as c√©lulas para criar os 3 volumes
   - Execute a c√©lula para verificar os volumes criados

3. **Fa√ßa upload dos arquivos CSV:**

   **Op√ß√£o A - Via UI (Recomendado):**

   - Navegue: Catalog ‚Üí smart_claims_dev ‚Üí 00_landing
   - Clique em cada volume ‚Üí Upload ‚Üí Selecione o CSV correspondente

   **Op√ß√£o B - Via C√≥digo Python:**
   - Ajuste os caminhos na c√©lula Python (Parte 4)
   - Execute a c√©lula para copiar os arquivos
   - Verifique as mensagens de sucesso/erro

4. **Verifique o upload (Parte 5):**
   - Execute a c√©lula Python de verifica√ß√£o
   - Confirme que os 3 arquivos aparecem nos volumes
   - Verifique os tamanhos dos arquivos

5. **Valida√ß√£o opcional (Parte 6):**
   - Descomente e execute o c√≥digo para ler os CSV como DataFrames
   - Verifique previews e contagens de registros
   - Valide a estrutura dos dados

### Resultado Esperado

Ap√≥s a execu√ß√£o bem-sucedida, voc√™ ter√°:

- ‚úÖ **3 volumes criados** no schema `00_landing`:
  - `claims_volume`
  - `customers_volume`
  - `policies_volume`

- ‚úÖ **3 arquivos CSV carregados** nos volumes:
  - `claims.csv` ‚Üí `claims_volume/claims.csv` (~2.8 MB)
  - `customers.csv` ‚Üí `customers_volume/customers.csv` (~450 KB)
  - `policies.csv` ‚Üí `policies_volume/policies.csv` (~890 KB)

- ‚úÖ **Estrutura pronta** para pr√≥xima etapa:
  - Arquivos organizados e governados
  - Base para cria√ß√£o de tabelas Delta na camada Bronze
  - Dados validados e acess√≠veis via Spark

### Estrutura Final da Task_002

```text
smart_claims_dev (CATALOG)
  ‚îî‚îÄ‚îÄ 00_landing (SCHEMA)
      ‚îú‚îÄ‚îÄ claims_volume (VOLUME)
      ‚îÇ   ‚îî‚îÄ‚îÄ claims.csv (12.990 registros de sinistros)
      ‚îú‚îÄ‚îÄ customers_volume (VOLUME)
      ‚îÇ   ‚îî‚îÄ‚îÄ customers.csv (7.060 registros de clientes)
      ‚îî‚îÄ‚îÄ policies_volume (VOLUME)
          ‚îî‚îÄ‚îÄ policies.csv (12.230 registros de ap√≥lices)
```

### Exemplos de Uso dos Volumes

#### Caso de Uso 1: Valida√ß√£o de Dados

```python
# Ler arquivo do volume e validar
df = spark.read.csv("/Volumes/smart_claims_dev/00_landing/claims_volume/claims.csv", 
                    header=True, inferSchema=True)
# Verificar qualidade dos dados antes de processar
df.filter(df.total.isNull()).count()  # Verificar valores nulos
```

#### Caso de Uso 2: Prepara√ß√£o para Bronze

```python
# Os arquivos nos volumes ser√£o usados na pr√≥xima task para criar tabelas Delta
# na camada 01_bronze, preservando os dados brutos imutavelmente
```

#### Caso de Uso 3: Auditoria e Rastreabilidade

```python
# Listar hist√≥rico de arquivos recebidos
# Cada arquivo no volume representa um lote de dados recebidos
# √ötil para auditoria e compliance
```

### Pr√≥ximos Passos

Ap√≥s concluir a Task_002, as pr√≥ximas etapas incluem:

- **Task_003**: Criar tabelas Delta na camada `01_bronze` a partir dos arquivos CSV nos volumes
- **Task_004**: Implementar transforma√ß√µes e valida√ß√µes (Bronze ‚Üí Silver)
- **Task_005**: Criar agregados e modelos para consumo final (Silver ‚Üí Gold)

---

## üìö Documenta√ß√£o Adicional

Para mais detalhes sobre comandos de Catalog e Unity Catalog, consulte:

- [`EXPLICACAO_CATALOG.md`](EXPLICACAO_CATALOG.md) - Documenta√ß√£o completa sobre Unity Catalog

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Databricks** - Plataforma de analytics e processamento de dados
- **Unity Catalog** (Lakeflow) - Sistema de governan√ßa de dados unificada
- **Delta Lake** - Formato de armazenamento transacional em lakehouse
- **SQL** - Linguagem para cria√ß√£o e manipula√ß√£o de objetos

---

## üìù Notas

- Todos os scripts SQL s√£o **idempotentes** (usam `IF NOT EXISTS`), podendo ser executados m√∫ltiplas vezes sem erro
- O projeto est√° configurado para ambiente de **desenvolvimento** (`dev`)
- A estrutura pode ser replicada para outros ambientes (staging, prod) ajustando o nome do cat√°logo

---

## üë• Contribui√ß√£o

Este √© um projeto em desenvolvimento. Para contribuir:

1. Siga o padr√£o de nomenclatura estabelecido
2. Mantenha a documenta√ß√£o atualizada
3. Teste scripts em ambiente de dev antes de produ√ß√£o
